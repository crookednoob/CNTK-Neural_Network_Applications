{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNTK : Basic AutoEncoders(AE) with MNIST Data\n",
    "\n",
    "#An autoencoder is an artificial neural network used for unsupervised learning of efficient encodings. \n",
    "#In other words, they are used for lossy data-specific compression that is learnt automatically instead \n",
    "#of relying on human engineered features. The aim of an autoencoder is to learn a representation (encoding) \n",
    "#for a set of data, typically for the purpose of dimensionality reduction.\n",
    "\n",
    "#The autoencoders are very specific to the data-set on hand and are different from standard codecs such as \n",
    "#JPEG, MPEG standard based encodings. Once the information is encoded and decoded back to original dimensions \n",
    "#some amount of information is lost in the process. Given these encodings are specific to data, autoencoders \n",
    "#are not used for compression. However, there are two areas where autoencoders have been found very effective: \n",
    "#denoising and dimensionality reduction.\n",
    "\n",
    "#Autoencoders have attracted attention since they have long been thought to be a potential approach for \n",
    "#unsupervised learning. Truly unsupervised approaches involve learning useful representations without \n",
    "#the need for labels. Autoencoders fall under self-supervised learning, a specific instance of supervised \n",
    "#learning where the targets are generated from the input data.\n",
    "\n",
    "#GOAL - \n",
    "#Our goal is to train an autoencoder that compresses MNIST digits image to a vector of smaller dimension \n",
    "#and then restores the image. The MNIST data comprises of hand-written digits with little background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/MNIST-image.jpg\" width=\"300\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Figure 1\n",
    "Image(url=\"http://cntk.ai/jup/MNIST-image.jpg\", width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the relevant modules\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env()\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "isFast=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a CTF formatted text using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels_viz = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is data\\MNIST\n"
     ]
    }
   ],
   "source": [
    "# We search in two locations in the toolkit for the cached MNIST data set.\n",
    "data_found = False\n",
    "for data_dir in [os.path.join(\"..\", \"Examples\", \"Image\", \"DataSets\", \"MNIST\"),\n",
    "                 os.path.join(\"data\", \"MNIST\")]:\n",
    "    train_file = os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "    test_file = os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "        data_found = True\n",
    "        break\n",
    "\n",
    "if not data_found:\n",
    "    raise ValueError(\"Please generate the data by completing CNTK 103 Part A\")\n",
    "print(\"Data directory is {0}\".format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/SimpleAEfig.jpg\" width=\"200\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Creation -- Simple AE\n",
    "# Figure 2\n",
    "Image(url=\"http://cntk.ai/jup/SimpleAEfig.jpg\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=784\n",
    "encoding_dim=32\n",
    "output_dim=input_dim\n",
    "\n",
    "def create_model(features):\n",
    "    with C.layers.default_options(init=C.glorot_uniform()):\n",
    "        #We scale the input pixel to 0-1 range\n",
    "        encode=C.layers.Dense(encoding_dim, activation=C.relu)(features/255.0)\n",
    "        decode=C.layers.Dense(input_dim, activation=C.sigmoid)(encode)\n",
    "        \n",
    "    return decode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test the model\n",
    "\n",
    "def train_and_test(reader_train, reader_test, model_func):\n",
    "    \n",
    "    ###########################\n",
    "    #Training Model\n",
    "    ###########################\n",
    "    \n",
    "    #Instantiate the input and label variables\n",
    "    input=C.input_variable(input_dim)\n",
    "    label=C.input_variable(input_dim)\n",
    "    \n",
    "    #Create the model func\n",
    "    model=model_func(input)\n",
    "    \n",
    "    #The labels for this network is same as the MNIST image\n",
    "    #Note: Inside the model we are scaling the input to 0-1 range\n",
    "    #Hence we rescale the label to the same range\n",
    "    #Will show how to use custom loss function\n",
    "    #loss = -(y*log(p) + (1-y)*log(1-p)) where p= model output and y= target\n",
    "    #We have normalized the input between 0-1. We are scaling the target to same range\n",
    "    \n",
    "    target=label/255.0\n",
    "    loss=-(target*C.log(model) + (1-target)*C.log(1-model))\n",
    "    label_error=C.classification_error(model, target)\n",
    "    \n",
    "    #training config\n",
    "    epoch_size=30000 #It is half the dataset size\n",
    "    minibatch_size=64\n",
    "    num_sweeps_to_train_with=5 if isFast else 100\n",
    "    num_samples_per_sweep= 60000\n",
    "    num_minibatches_to_train= (num_samples_per_sweep*num_sweeps_to_train_with)//minibatch_size\n",
    "    \n",
    "    #Instantiate the  trainer object to drive teh model training\n",
    "    lr_per_sample = [0.00003]\n",
    "    lr_schedule = C.learning_parameter_schedule_per_sample(lr_per_sample, epoch_size)\n",
    "    \n",
    "    #Momentum which is applied on every minibatch size = 64 samples\n",
    "    momentum_schedule=C.momentum_schedule(0.9126265014311797, minibatch_size)\n",
    "    \n",
    "    #We are using a variant of Adam optimizer which is known to work well on this dataset\n",
    "    learner= C.fsadagrad(model.parameters, lr=lr_schedule, momentum=momentum_schedule)\n",
    "    \n",
    "    #Instantiate the trainer\n",
    "    progress_printer=C.logging.ProgressPrinter(0)\n",
    "    trainer=C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "    \n",
    "    #Map the data streams to the input and labels\n",
    "    #Note: For AutoEncoders, input=label\n",
    "    input_map={\n",
    "        input : reader_train.streams.features,\n",
    "        label : reader_train.streams.features\n",
    "    }\n",
    "    \n",
    "    aggregate_metric=0\n",
    "    for i in range (num_minibatches_to_train):\n",
    "        #Read a minibatch from the training data file\n",
    "        data=reader_train.next_minibatch(minibatch_size, input_map= input_map)\n",
    "        \n",
    "        #Run the trainer on and perform model training\n",
    "        trainer.train_minibatch(data)\n",
    "        samples=trainer.previous_minibatch_sample_count\n",
    "        aggregate_metric+= trainer.previous_minibatch_evaluation_average*samples\n",
    "        \n",
    "    train_error=(aggregate_metric*100.0)/(trainer.total_number_of_samples_seen)\n",
    "    print(\"Average Training Error: {0:0.2f}%\".format(train_error))\n",
    "    \n",
    "    ##############################################################################\n",
    "    #Testing the model\n",
    "    #Note: We use a test file reader to read data different from a training data\n",
    "    ##############################################################################\n",
    "    \n",
    "    #Test data for trained model\n",
    "    test_minibatch_size=32\n",
    "    num_samples=10000\n",
    "    num_minibatches_to_test=num_samples/test_minibatch_size\n",
    "    test_result = 0.0\n",
    "    \n",
    "    #Test error metric calculation\n",
    "    metric_numer=0\n",
    "    metric_denom=0\n",
    "    \n",
    "    test_input_map= {\n",
    "        input : reader_test.streams.features,\n",
    "        label : reader_test.streams.features\n",
    "    }\n",
    "    \n",
    "    for i in range (0, int(num_minibatches_to_test)):\n",
    "        \n",
    "        #We are loading the test data in batches specified by the test_minibatch_size\n",
    "        #Each data point in a minibatch is a MNIST image of 784 dimensions\n",
    "        #with one pixel per dimension taht will encode/ decode with the trained model\n",
    "        \n",
    "        data = reader_test.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        \n",
    "        #Specifying the  mapping of input variables in the model to the actual minibatch data to be tested with\n",
    "        eval_error=trainer.test_minibatch(data)\n",
    "        \n",
    "        #minibatch data to be trained with\n",
    "        metric_numer+=np.abs(eval_error*test_minibatch_size)\n",
    "        metric_denom+=test_minibatch_size\n",
    "        \n",
    "    #Average of evaluation errors  of all test minibatches\n",
    "    test_error= (metric_numer*100.0)/ (metric_denom)\n",
    "    print(\"Average test error: {0:0.2f}%\".format(test_error))\n",
    "    \n",
    "    return model, train_error, test_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\cntk\\learners\\__init__.py:340: RuntimeWarning: When providing the schedule as a number, epoch_size is ignored\n",
      "  warnings.warn('When providing the schedule as a number, epoch_size is ignored', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per 1 samples: 3e-05\n",
      "      544        544      0.877      0.877            64\n",
      "      544        544      0.866       0.86           192\n",
      "      544        543      0.869      0.871           448\n",
      "      542        541      0.869      0.868           960\n",
      "      538        533       0.84      0.813          1984\n",
      "      496        455      0.731      0.625          4032\n",
      "      385        276      0.566      0.404          8128\n",
      "      303        221       0.44      0.315         16320\n",
      "      250        197      0.341      0.242         32704\n",
      "      208        167      0.258      0.175         65472\n",
      "      173        138      0.183      0.107        131008\n",
      "      142        110      0.116     0.0495        262080\n",
      "Average Training Error: 10.57%\n",
      "Average test error: 3.04%\n"
     ]
    }
   ],
   "source": [
    "num_label_classes=10\n",
    "reader_train=create_reader(train_file, True, input_dim, num_label_classes)\n",
    "reader_test=create_reader(test_file, False, input_dim, num_label_classes)\n",
    "model, simple_ae_train_error, simple_ae_test_error= train_and_test(reader_train, reader_test, model_func=create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image Statistics: \n",
      "Max: 255.00, Median: 0.00, Mean: 24.07, Min: 0.00\n",
      "Decoded Image Statistics: \n",
      "Max: 251.79, Median: 0.43, Mean: 26.11, Min: 0.00\n"
     ]
    }
   ],
   "source": [
    "#Visualize simple AE results\n",
    "\n",
    "#Read some data to run the evaluation\n",
    "num_label_classes=10\n",
    "reader_eval= create_reader(test_file, False, input_dim, num_label_classes)\n",
    "\n",
    "eval_minibatch_size=50\n",
    "eval_input_map = { input : reader_eval.streams.features}\n",
    "\n",
    "eval_data=reader_eval.next_minibatch(eval_minibatch_size, input_map=eval_input_map)\n",
    "\n",
    "img_data= eval_data[input].asarray()\n",
    "\n",
    "#select a random image\n",
    "np.random.seed(0)\n",
    "idx=np.random.choice(eval_minibatch_size)\n",
    "\n",
    "orig_image= img_data[idx, :,:]\n",
    "decoded_image= model.eval(orig_image)[0]*255\n",
    "\n",
    "#Print image statistics\n",
    "def print_img_stats(img, text):\n",
    "    print(text)\n",
    "    print(\"Max: {0:.2f}, Median: {1:.2f}, Mean: {2:.2f}, Min: {3:.2f}\".format(np.max(img), np.median(img),\n",
    "                                                                             np.mean(img), np.min(img)))\n",
    "    \n",
    "#print original image\n",
    "print_img_stats(orig_image, \"Original Image Statistics: \")\n",
    "\n",
    "#print decoded image\n",
    "print_img_stats(decoded_image, \"Decoded Image Statistics: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the original and decoded image\n",
    "\n",
    "#Define a helper function to plot the images\n",
    "def plot_image_pair(img1, text1, img2, text2):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(6,6))\n",
    "    \n",
    "    axes[0].imshow(img1, cmap=\"gray\")\n",
    "    axes[0].set_title(text1)\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    axes[1].imshow(img2, cmap=\"gray\")\n",
    "    axes[1].set_title(text2)\n",
    "    axes[1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEtdJREFUeJzt3XuQV+V9x/HPV26CwIIgIovANIiAqKCSKNJ6o6KODLZTbUs6KlWLrSbVVuN4DRgvMWPq1HY0GZM2Gh1tJFovxaZQNFbFS53WQVoviOgigpHrIssC8vSPc0hO9/me3d8P9vrs+zWzw4/v+Z7L/vbsd5/feZ7zHAshCADQ9R3Q0QcAAGgdFHQASAQFHQASQUEHgERQ0AEgERR0AEgEBb1CZnaDmf2otXMr2FYws7GtsS2gLZjZqWa2pr3XRaxbFnQzu9jMlpvZdjNbZ2b3m9mg5tYJIdwRQri0ku1Xk7s/zOwFM2vz/aBzMrPVZtZgZvVmttnMXjGzy80smd9rGjTVSeYHXykz+2tJd0m6VlKNpBMljZa02Mx6l6zTs/2OEKjKrBDCAGXn8HclXSfpxx17SOgo3aqgm9lASQskfSOE8K8hhF0hhNWSLlD2C/Ened58M1toZg+b2VZJF+exhwvbutDMPjKzDWZ2c95amlFY/+H89Zi8lXGRmX1sZp+b2Y2F7XzVzJblLaxPzezvy/6wtPC9nWpma8zsW2b2Wb6t88zsHDN7z8w2mtkNle7XzM40s3fNbIuZ3Wdmvyx+GjCzPzWz/zWzTWb2CzMbXe0xo/WEELaEEJ6W9IeSLjKzSZJkZn3M7O783FtvZj8ws7571zOz2Wb232a21cw+MLOz8vgIM3s6P29WmtllhXX6mtlP8p/9/0iaWjyWfN2fm9mvzOxDM/tmpes2J/+9ejz/vazPP2WPM7Pr83O+zszOLOTPzc/RejNbZWbzmmzvW/m5v9bMLi1+GmjpfeusulVBlzRN0oGSnigGQwjbJD0n6XcL4dmSFkoaJOmRYr6ZTZR0n6SvSzpMWUu/toV9T5d0pKQzJN1iZhPy+JeSrpY0VNJJ+fK/qPL72mu4su+vVtItkh5Q9kfqeEm/ne/3t1rar5kNVfa9Xy9piKR3lb13ypefJ+kGSb8v6RBJ/yHp0X08ZrSiEMLrktYo+3lL2afRcZImSxqr35wbMrOvSnpI2afVQZJ+R9LqfL1H8+2MkPQHku4wszPyZd+W9JX8a6aki/buP7/c84ykt/J9nSHpKjOb2dK6FZol6aeSBkv6L0m/UFbHaiXdKumHhdzPJJ0raaCkuZLuMbPj8uM8S9JfSZqRvy+nNNlP6fvWqYUQus2XsuK2rmTZdyUtzl/Pl/Rik+XzJT2cv75F0qOFZf0k7ZQ0w8kdIylIGlnIf13SH5Ucx1WSniz8P0gaW5L7gqRL89enSmqQ1CP//4B83a8V8t+UdF5L+5V0oaRlhWUmqa6wr+ckXVJYfoCk7ZJGd/TPuDt9KSu+M5z4q5JuzH9uX0j6SmHZSZI+zF//UNI9zvqHK/uDP6AQu1PST/LXqySdVVj2Z5LW5K+/JunjJtu7XtI/trRuyff46/M//71aXFg2S9I255wfVLKtf5b0l/nrf5B0Z2HZ2L37aul968xf3e3a8OeShppZzxDC7ibLDsuX71XXzHZGFJeHELab2YYW9r2u8Hq7pP6SZGbjJP2NpBOU/WHoqazw7osNIYQv89cN+b/rC8sbKtxv0+8v2P8fiTBa0t+a2fcLMVPWivloH48dradW0kZln576SXrTzPYuM0k98teHS1rkrD9C0sYQQn0h9pGyc2Xv8romy/YaLWmEmW0uxHoo+xTX0rqVaHo+f+6c8/0lbTazs5V9IhinrNHRT9LywnH8Z2FbxWNq6X3rtLrbJZdlkhqVXSr4NTM7SNLZkv69EG5uGspPJY0srN9X2aWJfXG/pHckHRFCGKjsUoY1v0qraG6/Tb8/K/5f2ck/L4QwqPDVN4TwSjscN5phZlOVFfSXlDVQGiQdVfg51YQQ+ufpdcoufTS1VtLBZjagEBsl6ZP89afK/hgUl+1Vp6wlWzw3BoQQzqlg3VZjZn0k/VzS3ZIODSEMUvbHyz3HmxxTS+9bp9WtCnoIYYuyTtG/M7OzzKyXmY2R9Liy64U/rXBTCyXNMrNpeUfiAu17ER4gaaukbWY2XtKf7+N2WnO//yLp6LxTtaekK5Rdn9/rB5KuN7OjJMnMaszs/HY6bjjMbKCZnSvpMWWX+5aHEPYo60e5x8yG5Xm1hevZP5Y018zOMLMD8mXjQwh1kl6RdKeZHWhmx0i6RL/pS/qZsp//YDMbKekbhUN5XdJWM7su7wDtYWaT8j80La3bmnpL6iPpV5J25631MwvLf5Z/7xPMrJ8K18creN86rW5V0CUphPA9Za3Ru5UVtNeUtSrOCCE0VriNFcpOxMeU/aWvV9YBU9H6TVwjaU6+jQck/dM+bGNflO43hPC5pPMlfU/SBkkTlX08bcyXP6ms0+gxy0YBva3sEw7a3zNmVq/sHL5R2WW0uYXl10laKenV/Ge1RFnnvELWgTpX0j2Stkj6pbJLJpL0x8r6f9ZKelLSt0MIi/NlC5RdKvlQ0r+p0BDKL3/MUtaZ+KGy1u6PlA0caHbd1pRfLvqmssK9Sdm5/nRh+XOS7pX0vLL3Z1m+aO/vcOn71plZfsEf+8HM+kvarOzyxYcdfTytLR+5sEbS10MIz3f08QCtLR919rakPk7/WpfR7VrorcXMZplZv/z6+93KOltWd+xRtR4zm2lmg/JrkXuvr7/awYcFtBoz+z0z621mg5V94nymKxdziYK+P2Yr+zi6VtIRyoYhpvRx5yRJHyj7yDxL2XDHhuZXAbqUecqusX+gbJhme/VftRkuuQBAImihA0AiKOgAkIh2vVPUzLi+gzYVQmiPm7IinNtoa5Wc27TQASARFHQASAQFHQASQUEHgERQ0AEgERR0AEgEBR0AEkFBB4BEUNABIBEUdABIBAUdABJBQQeARFDQASARFHQASES7Tp8LoHMwi2di9WKS1LNnXCb69evn5u7cuTOK9erVK4rt2LHDXX/Pnj1R7Msvv3RzvaetdfcnsNFCB4BEUNABIBEUdABIBAUdABJBQQeARDDKpUqHHnqoG58/f34Uu/zyy91cryf+kUceiWI333yzu/7q1avLDxDdljdKpW/fvm7ugAEDotjgwYPd3NmzZ0exIUOGuLkTJkyIYt4ola1bt7rrP/HEE1Hs+eefd3O3bdtW0b7akveed+RIG1roAJAICjoAJIKCDgCJoKADQCLoFG2G1wG6ZMkSN3fixIlRzLuNucycOXOi2IMPPujm0inavXm34pfF+/Tp4+aOHz8+ik2bNs3NnTx5chQ74ogj3FyvE9aLlXWKrlmzJoq9//77bq4XL/ud6y5TAtBCB4BEUNABIBEUdABIBAUdABJBQQeARDDKpRm33XZbFBs1apSb+8ADD0SxTZs2ublXX311FPMeAnDttde665eNtEF6qnkQhTeapKamxs31HlBRdtv8Cy+8EMVWrVrl5vbv3z+KDR8+PIo1NDS463ujX3bv3u3m9ujRI4rt2rXLzd1fZe/5AQfEbeKyETXVjHrbV7TQASARFHQASAQFHQASQUEHgETQKdqMLVu2RLFLLrnEzV24cGHF262trY1i559/fhQru227d+/eUcx72jq6vv2db7uxsdGNe7fNv/322xUfQ9n0AwMHDoxiRx11VBQbN26cu/7GjRuj2MiRI93czz77LIqVdYpWM0+619FZ1inaHh2d1aCFDgCJoKADQCIo6ACQCAo6ACSCgg4AibD2nPjdzLrHLPMt8EavLF68OIqdfPLJ7vregwhee+21/T+wBIQQ/OEIbaw9z21vFIbkjzwpy/Vumy8bseGNECnb7kEHHRTFTjvttCh2zDHHuOt702W88cYbbu67774bxTZs2ODmenWumtv5y3jvWVuNfKnk3KaFDgCJoKADQCIo6ACQCAo6ACSCW/87wJQpU6JYWQco0FRZp5s3b3hZx18184Z7HYplt/4PHTo0ih1yyCFRbPDgwRUf17Zt29zcsrjH6+ispvOzmsEjZe95ewxAoYUOAImgoANAIijoAJAICjoAJIKCDgCJYJRLBxg2bFhFecuXL3fjK1eubM3DQSK8URRlIyuqeXCGl1s2QmTy5MlRbObMmVHMexCGJH388cdRbN26dW6uN6qn7LiqGWHi5VbzgIz2nE6lKVroAJAICjoAJIKCDgCJoKADQCLoFO0Al112WUV569evd+Nlcz6je6tmzu/9vZV9zJgxbu706dOj2HHHHRfFvHnPJb9TdPv27W6uN6d7mWo6jL2pFTqyo7MatNABIBEUdABIBAUdABJBQQeARFDQASARjHJpQ2PHjnXjU6dOrWj9mpoaN37iiSdGsbJpAr744ouK9gWU3TbvnYdz5sxxc88555woduCBB0Yx77Z9Sdq5c2cUGzFihJu7efPmirfrjVLZunVrxbldBS10AEgEBR0AEkFBB4BEUNABIBF0irahsjmfvaege8o6T19++eUo9tBDD7m5d911VxR75513Kto/ur5q5jgvu5V+4sSJUWzSpElurrcNr7O1rENy+PDhUazs98ibo9ybOkCSVqxYEcXK3hs6RQEAHY6CDgCJoKADQCIo6ACQCDpF21BZx8+SJUuimDdn9MEHH1zxvi688EI3Pnr06Ch27rnnurll804jPV7HX9mdoocffnhF60t+56PXAevdESpJ06ZNi2KHHXaYmztgwIAo9uyzz7q5q1atimLenaZdHS10AEgEBR0AEkFBB4BEUNABIBEUdABIhLXnba5m1nXvqW1j48ePj2JTpkxxc6+66qoodsIJJ1S8r7feesuNe3NZr1u3ruLtdgYhBP8x920shXO7V69ebry2tjaKld36782/37NnPJhuzJgx7vqzZs2KYkcffbSb682zvnTpUjf3jjvuiGLvvfeem+vNqd4ZpgOo5NymhQ4AiaCgA0AiKOgAkAgKOgAkglv/OwlvjvKyecsXLVoUxZYtW+bmHnnkkVHs2GOPdXN79+7d3CEicd784pLfMb527dqKt+t1itbX17u5p59+ekXrl8XL5k4fPHhwc4eYDFroAJAICjoAJIKCDgCJoKADQCIo6ACQCEa5dEFbtmyJYg0NDR1wJEjJnj173HhjY+N+bdd7wEXZaJS+fftGsbLRN97x1tTUVLzdsgd6eMdbpjNMCVBECx0AEkFBB4BEUNABIBEUdABIBJ2indiIESPc+Lx586LYhAkTKt6u9wR0yZ/LGl1HNZ15bbXdHj16RLFhw4ZFsenTp7vrjxo1Kop5855LfmftihUr3Nz169dHsbIOTS9e9h7QKQoAaBMUdABIBAUdABJBQQeARFDQASARjHLpJM4+++wotmDBAjf3+OOPr3i73ogWb1+StGHDhoq3i45TzS3rZbnVjM7wttGrVy83d+zYsVHsmmuuiWLegywkqX///lGsbEoC78EbL774optbV1cXxcqmFPB0ttEsZWihA0AiKOgAkAgKOgAkgoIOAImgU7QNzZ07143ffvvtUcx7Knnv3r0r3tfjjz/uxm+66aYotnLlyoq3i47ldXSW3YbudVSWdYp6uWUdf97c5aeccoqbe8UVV0SxqVOnRrGePf3Ss3Pnzii2ceNGN3fp0qVRbMmSJW5ufX19FCvrbO0qHaAeWugAkAgKOgAkgoIOAImgoANAIijoAJAIRrm0kosvvjiK3X///W5u2W3Tlbrtttui2He+8x03d/fu3fu1L3SsakZceLllI6UGDRpU8b4mTpwYxa688ko3d9KkSVGsbESLZ/v27VFs8eLFbu69994bxTZv3uzmerf5d+XRLGVooQNAIijoAJAICjoAJIKCDgCJoFO0lcycOTOK7W/npzdFgCTdeuutUayauZ3R+VTzVPmyW9a9DvDGxkY3d9OmTVFsyJAhbm5tbW0U27Fjh5vrHVtDQ0MUW79+vbv+okWLoth9993n5n7yySdRrGwQQIodoB5a6ACQCAo6ACSCgg4AiaCgA0AiKOgAkAhGubSSl156KYpdcMEFbq7XOz9jxowoVvYgirJRDui6ykZhlI1+8XjnRdkol127dkWxspFSTz31VBRbs2aNm+uNiPFu53/zzTfd9evq6qKY99ALyf9+u8toljK00AEgERR0AEgEBR0AEkFBB4BEWHt2IphZ9+6xQJsLIVTei9iKUji3q+mArWaqgmp0907N5lRybtNCB4BEUNABIBEUdABIBAUdABJBQQeARHDrPwBJ1Y0wYTRK50QLHQASQUEHgERQ0AEgERR0AEgEBR0AEkFBB4BEUNABIBEUdABIBAUdABJBQQeARFDQASARFHQASAQFHQASQUEHgERQ0AEgEca8xgCQBlroAJAICjoAJIKCDgCJoKADQCIo6ACQCAo6ACSCgg4AiaCgA0AiKOgAkAgKOgAkgoIOAImgoANAIijoAJAICjoAJIKCDgCJoKADQCIo6ACQCAo6ACSCgg4AiaCgA0AiKOgAkAgKOgAkgoIOAIn4P8/pWX7fUIk5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img1= orig_image.reshape(28,28)\n",
    "text1=\"Original Image\"\n",
    "\n",
    "img2=decoded_image.reshape(28,28)\n",
    "text2=\"Decoded Image\"\n",
    "\n",
    "plot_image_pair(img1, text1, img2, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/DeepAEfig.jpg\" width=\"500\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Creation -- Deep AE\n",
    "\n",
    "# Figure 3\n",
    "Image(url=\"http://cntk.ai/jup/DeepAEfig.jpg\", width=500, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=784\n",
    "encoding_dims=[128, 64, 32]\n",
    "decoding_dims=[64, 128]\n",
    "\n",
    "encoded_model= None\n",
    "\n",
    "def create_deep_model(features):\n",
    "    with C.layers.default_options(init= C.layers.glorot_uniform()):\n",
    "        encode= C.element_times(C.constant(1.0/255.0), features)\n",
    "        \n",
    "        for encoding_dim in encoding_dims:\n",
    "            encode= C.layers.Dense(encoding_dim, activation=C.relu)(encode)\n",
    "            \n",
    "        global encoded_model\n",
    "        encoded_model= encode\n",
    "        \n",
    "        decode=encode\n",
    "        for decoding_dim in decoding_dims:\n",
    "            decode = C.layers.Dense(decoding_dim, activation=C.relu)(decode)\n",
    "            \n",
    "        decode = C.layers.Dense(input_dim, activation=C.sigmoid)(decode)\n",
    "        \n",
    "        return decode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\cntk\\learners\\__init__.py:340: RuntimeWarning: When providing the schedule as a number, epoch_size is ignored\n",
      "  warnings.warn('When providing the schedule as a number, epoch_size is ignored', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per 1 samples: 3e-05\n",
      "      544        544       0.75       0.75            64\n",
      "      544        544      0.771      0.781           192\n",
      "      544        543       0.78      0.786           448\n",
      "      543        542      0.789      0.797           960\n",
      "      530        518      0.863      0.933          1984\n",
      "      415        303       0.77      0.679          4032\n",
      "      315        216      0.624       0.48          8128\n",
      "      258        203        0.5      0.376         16320\n",
      "      215        171      0.367      0.234         32704\n",
      "      176        137      0.251      0.136         65472\n",
      "      145        114      0.165     0.0791        131008\n",
      "      122       98.7      0.107     0.0483        262080\n",
      "Average Training Error: 9.80%\n",
      "Average test error: 3.17%\n"
     ]
    }
   ],
   "source": [
    "num_label_classes = 10\n",
    "reader_train = create_reader(train_file, True, input_dim, num_label_classes)\n",
    "reader_test = create_reader(test_file, False, input_dim, num_label_classes)\n",
    "\n",
    "model, deep_ae_train_error, deep_ae_test_error = train_and_test(reader_train,\n",
    "                                                                reader_test,\n",
    "                                                                model_func = create_deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image Statistics: \n",
      "Max: 255.00, Median: 0.00, Mean: 24.07, Min: 0.00\n",
      "Decoded Image Statistics: \n",
      "Max: 249.46, Median: 0.03, Mean: 23.85, Min: 0.00\n"
     ]
    }
   ],
   "source": [
    "#Visualize Deep AE results\n",
    "\n",
    "#Run the same image as the simple autoencoder through the deep encoder\n",
    "orig_image= img_data[idx, :, :]\n",
    "decoded_image=model.eval(orig_image)[0]*255\n",
    "\n",
    "#Print image statistics\n",
    "def print_img_stats(img, text):\n",
    "    print(text)\n",
    "    print(\"Max: {0:.2f}, Median: {1:.2f}, Mean: {2:.2f}, Min: {3:.2f}\".format(np.max(img), np.median(img),\n",
    "                                                                             np.mean(img), np.min(img)))\n",
    "    \n",
    "#print original image\n",
    "print_img_stats(orig_image, \"Original Image Statistics: \")\n",
    "\n",
    "#print decoded image\n",
    "print_img_stats(decoded_image, \"Decoded Image Statistics: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the original and decoded image\n",
    "\n",
    "#Define a helper function to plot the images\n",
    "def plot_image_pair(img1, text1, img2, text2):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(6,6))\n",
    "    \n",
    "    axes[0].imshow(img1, cmap=\"gray\")\n",
    "    axes[0].set_title(text1)\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    axes[1].imshow(img2, cmap=\"gray\")\n",
    "    axes[1].set_title(text2)\n",
    "    axes[1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the original and the decoded image\n",
    "img1 = orig_image.reshape(28,28)\n",
    "text1 = 'Original image'\n",
    "\n",
    "img2 = decoded_image.reshape(28,28)\n",
    "text2 = 'Decoded image'\n",
    "\n",
    "plot_image_pair(img1, text1, img2, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will explore how we can compare one to another and also show how to extract an encoded input for a given input. \n",
    "#For visualizing high dimension data in 2D, t-SNE (t-distributed Stochastic Neighbor Embedding) \n",
    "#is probably one of the best methods. However, it typically requires relatively low-dimensional data. \n",
    "#So a good strategy for visualizing similarity relationships in high-dimensional data is to encode data into \n",
    "#a low-dimensional space (e.g. 32 dimensional) using an autoencoder first, extract the encoding of the input \n",
    "#data followed by using t-SNE for mapping the compressed data to a 2D plane.\n",
    "\n",
    "#We will use the deep autoencoder outputs to: - Compare two images and - Show how we can retrieve an encoded (compressed) data.\n",
    "\n",
    "#t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities \n",
    "#and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding \n",
    "#and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we \n",
    "#can get different results.\n",
    "\n",
    "#Read some data to get the image data and corresponding labels\n",
    "num_label_classes=10\n",
    "reader_viz=create_reader(test_file, False, input_dim, num_label_classes)\n",
    "\n",
    "image=  C.input_variable(input_dim)\n",
    "image_label = C.input_variable(num_label_classes)\n",
    "\n",
    "viz_minibatch_size=50\n",
    "\n",
    "viz_input_map={\n",
    "    image : reader_viz.streams.features,\n",
    "    image_label : reader_viz.streams.labels_viz\n",
    "}\n",
    "\n",
    "viz_data=reader_eval.next_minibatch(viz_minibatch_size, input_map=viz_input_map)\n",
    "\n",
    "img_data= viz_data[image].asarray()\n",
    "imglabel_raw= viz_data[image_label].asarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [7, 35, 37, 43, 45]\n",
      "3: [12, 42]\n",
      "9: [4, 5, 8, 13, 18, 25]\n"
     ]
    }
   ],
   "source": [
    "#Map the image labels into indices in the minibatch array\n",
    "img_labels= [np.argmax(imglabel_raw[i,:,:]) for i in range(0, imglabel_raw.shape[0])]\n",
    "\n",
    "from collections import defaultdict\n",
    "label_dict= defaultdict(list)\n",
    "for img_idx, img_label in enumerate(img_labels):\n",
    "    label_dict[img_label].append(img_idx)\n",
    "    \n",
    "#Print indices corresponding to 3 digits\n",
    "randIdx = [1,3,9]\n",
    "for i in randIdx:\n",
    "    print(\"{0}: {1}\".format(i, label_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will compute cosine distance between two images using scipy\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "def image_pair_cosine_distance(img1, img2):\n",
    "    if img1.size!=img2.size:\n",
    "        raise ValueEerror(\"Two images need to be of same dimension\")\n",
    "    return 1-spatial.distance.cosine(img1, img2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between two original image: 0.534\n",
      "Distance between two decoded image: 0.723\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets compute the distance between the two images of same number\n",
    "\n",
    "digit_of_interest=6\n",
    "\n",
    "digit_index_list= label_dict[digit_of_interest]\n",
    "\n",
    "if len(digit_index_list)<2:\n",
    "    print(\"Need atleast two images to compare\")\n",
    "else:\n",
    "    imgA = img_data[digit_index_list[0],:,:][0]\n",
    "    imgB = img_data[digit_index_list[1],:,:][0]\n",
    "    \n",
    "    #print distance between original image\n",
    "    imgA_B_dist= image_pair_cosine_distance(imgA, imgB)\n",
    "    print(\"Distance between two original image: {0:.3f}\".format(imgA_B_dist))\n",
    "    \n",
    "    #Plot the two images\n",
    "    img1=imgA.reshape(28,28)\n",
    "    text1='Original Image 1'\n",
    "    \n",
    "    img2=imgB.reshape(28,28)\n",
    "    text2='Original Image 2'\n",
    "    \n",
    "    plot_image_pair(img1, text1, img2, text2)\n",
    "    \n",
    "    #Decode the encoded stream\n",
    "    imgA_decoded= model.eval([imgA])[0]\n",
    "    imgB_decoded= model.eval([imgB])[0]\n",
    "    imgA_B_decoded_list = image_pair_cosine_distance(imgA_decoded, imgB_decoded)\n",
    "    \n",
    "    #Print the distance between decoded image \n",
    "    print(\"Distance between two decoded image: {0:.3f}\".format(imgA_B_decoded_list))\n",
    "    \n",
    "    #Plot the two images\n",
    "    #Plot the original and decoded image\n",
    "    img1=imgA_decoded.reshape(28,28)\n",
    "    text1='Decoded Image 1'\n",
    "    \n",
    "    img2=imgB_decoded.reshape(28,28)\n",
    "    text2='Decoded Image 2'\n",
    "    \n",
    "    plot_image_pair(img1, text1, img2, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: The cosine distance between the original images comparable to the distance between the corresponding decoded images. \n",
    "#A value of 1 indicates high similarity between the images and 0 indicates no similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0350858990389566\n"
     ]
    }
   ],
   "source": [
    "#Simple auto encoder test error\n",
    "print(simple_ae_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1667083137644787\n"
     ]
    }
   ],
   "source": [
    "#Deep auto encoder test error\n",
    "print(deep_ae_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
